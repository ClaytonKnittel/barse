/// The bit width of numbers generated by the hasher.
#[cfg(feature = "multithreaded")]
pub const HASH_BITS: u32 = 15;
/// The bit width of numbers generated by the hasher.
#[cfg(not(feature = "multithreaded"))]
pub const HASH_BITS: u32 = 20;
pub const TABLE_SIZE: usize = 1 << HASH_BITS;

#[cfg(feature = "multithreaded")]
pub const HASH_MAGIC: u64 = 0x10000200400002;
#[cfg(not(feature = "multithreaded"))]
pub const HASH_MAGIC: u64 = 0x800400001001;

#[cfg(any(test, not(target_feature = "avx2")))]
mod generic_hasher {
  use std::ptr::read_unaligned;

  use crate::{
    str_hash::{HASH_BITS, HASH_MAGIC},
    util::{unaligned_read_would_cross_page_boundary, unlikely},
  };

  fn read_str_to_u128_slow(s: &[u8]) -> u128 {
    s.iter()
      .take(16)
      .enumerate()
      .map(|(i, b)| (*b as u128) << (8 * i))
      .sum()
  }

  /// Finds the first occurrence of byte `NEEDLE` in `v`, and returns `v` with
  /// that byte and all higher-order bytes zeroed out.
  fn mask_above(v: u128, len: usize) -> u128 {
    v & 1u128.unbounded_shl(8 * len.min(16) as u32).wrapping_sub(1)
  }

  fn compress_u128_to_u64(v: u128) -> u64 {
    v as u64 ^ (v >> 64) as u64
  }

  fn scramble_u64(v: u64) -> u64 {
    v.wrapping_mul(HASH_MAGIC) >> (64 - HASH_BITS)
  }

  pub fn str_hash(bytes: &[u8]) -> u64 {
    let ptr = bytes.as_ptr();
    let v = if unlikely(unaligned_read_would_cross_page_boundary::<u128>(ptr)) {
      read_str_to_u128_slow(bytes)
    } else {
      unsafe { read_unaligned(ptr as *const u128) }
    };

    let v = mask_above(v, bytes.len());
    let v = compress_u128_to_u64(v);
    scramble_u64(v)
  }

  #[cfg(test)]
  mod tests {
    use googletest::prelude::*;

    use crate::str_hash::generic_hasher::mask_above;

    #[gtest]
    fn test_mask_char_and_above() {
      expect_eq!(
        mask_above(0x10_11_12_13_14_15_16_17, 5),
        0x00_00_00_13_14_15_16_17
      );
      expect_eq!(
        mask_above(0x10_11_12_13_14_15_16_17, 12),
        0x10_11_12_13_14_15_16_17
      );
    }
  }
}

#[cfg(target_feature = "avx2")]
pub fn str_hash(bytes: &[u8]) -> u64 {
  crate::str_hash_x86::str_hash_fast(bytes)
}

#[cfg(not(target_feature = "avx2"))]
pub fn str_hash(bytes: &[u8]) -> u64 {
  generic_hasher::str_hash(bytes)
}

#[cfg(test)]
mod tests {
  use googletest::prelude::*;
  use itertools::Itertools;
  use rand::{
    distr::{Distribution, Uniform},
    rngs::StdRng,
    Rng, SeedableRng,
  };

  use crate::str_hash::{generic_hasher, str_hash};

  #[gtest]
  fn test_str_hash_different_positions() {
    #[repr(align(4096))]
    struct PageAligned([u8; 8192]);

    let s = b"test;123";
    let mut page_aligned = PageAligned([0xa4; 8192]);
    // Aligned load
    page_aligned.0[0..8].copy_from_slice(s);
    // Cross cache line
    page_aligned.0[60..68].copy_from_slice(s);
    // Cross page boundary
    page_aligned.0[4093..4101].copy_from_slice(s);

    let expected_hash = str_hash(&"test;123".as_bytes()[0..4]);
    expect_eq!(str_hash(&page_aligned.0[0..4]), expected_hash);
    expect_eq!(str_hash(&page_aligned.0[60..64]), expected_hash);
    expect_eq!(str_hash(&page_aligned.0[4093..4097]), expected_hash);
  }

  #[gtest]
  fn test_str_hash_fuzz() {
    let mut rng = StdRng::seed_from_u64(0x4214931);
    let distr = Uniform::new(2, 50).unwrap();

    fn rand_u8_excluding_semicolon<R: Rng>(rng: &mut R) -> u8 {
      let distr = Uniform::new(0, 254).unwrap();
      let v = distr.sample(rng);
      if v >= b';' {
        v + 1
      } else {
        v
      }
    }

    for _ in 0..1000 {
      let rand_len = distr.sample(&mut rng);
      let str_bytes = (0..rand_len)
        .map(|_| rand_u8_excluding_semicolon(&mut rng))
        .chain(std::iter::once(b';'))
        .collect_vec();

      let fast_hash = str_hash(&str_bytes[..rand_len]);
      let slow_hash = generic_hasher::str_hash(&str_bytes[..rand_len]);
      assert_eq!(fast_hash, slow_hash);
    }
  }
}
